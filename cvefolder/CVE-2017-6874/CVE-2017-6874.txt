CVE Number : CVE-2017-6874
Commit Message : 
ucount: Remove the atomicity from ucount->count
Commit Details : 
Always increment/decrement ucount->count under the ucounts_lock.  The
increments are there already and moving the decrements there means the
locking logic of the code is simpler.  This simplification in the
locking logic fixes a race between put_ucounts and get_ucounts that
could result in a use-after-free because the count could go zero then
be found by get_ucounts and then be freed by put_ucounts.

A bug presumably this one was found by a combination of syzkaller and
KASAN.  JongWhan Kim reported the syzkaller failure and Dmitry Vyukov
spotted the race in the code.

Cc: stable@vger.kernel.org
Fixes: f6b2db1a3e8d ("userns: Make the count of user namespaces per user")
Reported-by: JongHwan Kim <zzoru007@gmail.com>
Reported-by: Dmitry Vyukov <dvyukov@google.com>
Reviewed-by: Andrei Vagin <avagin@gmail.com>
Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

Before patch : 
 	struct hlist_node node;
 	struct user_namespace *ns;
 	kuid_t uid;
 	atomic_t count;
 	atomic_t ucount[UCOUNT_COUNTS];
 };
 
 
 		new >ns = ns;
 		new >uid = uid;
 		atomic_set(&new >count, 0);
 
 		spin_lock_irq(&ucounts_lock);
 		ucounts = find_ucounts(ns, uid, hashent);
 			ucounts = new;
 		}
 	}
 	if (!atomic_add_unless(&ucounts >count, 1, INT_MAX))
 		ucounts = NULL;
 	spin_unlock_irq(&ucounts_lock);
 	return ucounts;
 }
 {
 	unsigned long flags;
 
 	if (atomic_dec_and_test(&ucounts >count)) {
 		spin_lock_irqsave(&ucounts_lock, flags);
 		hlist_del_init(&ucounts >node);
 		spin_unlock_irqrestore(&ucounts_lock, flags);
 
 		kfree(ucounts);
 	}
 }
 
 static inline bool atomic_inc_below(atomic_t *v, int u)
After patch : 
 	struct hlist_node node;
 	struct user_namespace *ns;
 	kuid_t uid;
 	int count;
 	atomic_t ucount[UCOUNT_COUNTS];
 };
 
 
 		new >ns = ns;
 		new >uid = uid;
 		new >count = 0;
 
 		spin_lock_irq(&ucounts_lock);
 		ucounts = find_ucounts(ns, uid, hashent);
 			ucounts = new;
 		}
 	}
 	if (ucounts >count == INT_MAX)
 		ucounts = NULL;
 	else
 		ucounts >count  = 1;
 	spin_unlock_irq(&ucounts_lock);
 	return ucounts;
 }
 {
 	unsigned long flags;
 
 	spin_lock_irqsave(&ucounts_lock, flags);
 	ucounts >count  = 1;
 	if (!ucounts >count)
 		hlist_del_init(&ucounts >node);
 	else
 		ucounts = NULL;
 	spin_unlock_irqrestore(&ucounts_lock, flags);
 
 	kfree(ucounts);
 }
 
 static inline bool atomic_inc_below(atomic_t *v, int u)
