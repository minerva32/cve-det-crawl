CVE Number : CVE-2011-0999
Commit Message : 
thp: prevent hugepages during args/env copying into the user stack
Commit Details : 
Transparent hugepages can only be created if rmap is fully
functional. So we must prevent hugepages to be created while
is_vma_temporary_stack() is true.

This also optmizes away some harmless but unnecessary setting of
khugepaged_scan.address and it switches some BUG_ON to VM_BUG_ON.

Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
Acked-by: Rik van Riel <riel@redhat.com>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

Before patch : 
 	  (transparent_hugepage_flags &					\
 	   (1<<TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG) &&			\
 	   ((__vma) >vm_flags & VM_HUGEPAGE))) &&			\
 	 !((__vma) >vm_flags & VM_NOHUGEPAGE))
 #define transparent_hugepage_defrag(__vma)				\
 	((transparent_hugepage_flags &					\
 	  (1<<TRANSPARENT_HUGEPAGE_DEFRAG_FLAG)) ||			\
 	/* VM_PFNMAP vmas may have vm_ops null but vm_file set */
 	if (!vma >anon_vma || vma >vm_ops || vma >vm_file)
 		goto out;
 	VM_BUG_ON(is_linear_pfn_mapping(vma) || is_pfn_mapping(vma));
 
 	pgd = pgd_offset(mm, address);
 		if ((!(vma >vm_flags & VM_HUGEPAGE) &&
 		     !khugepaged_always()) ||
 		    (vma >vm_flags & VM_NOHUGEPAGE)) {
 			progress  ;
 			continue;
 		}
 
 		/* VM_PFNMAP vmas may have vm_ops null but vm_file set */
 		if (!vma >anon_vma || vma >vm_ops || vma >vm_file) {
 			khugepaged_scan.address = vma >vm_end;
 			progress  ;
 			continue;
 		}
 		VM_BUG_ON(is_linear_pfn_mapping(vma) || is_pfn_mapping(vma));
 
 		hstart = (vma >vm_start   ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;
 		hend = vma >vm_end & HPAGE_PMD_MASK;
 		if (hstart >= hend) {
 			progress  ;
 			continue;
 		}
 		if (khugepaged_scan.address < hstart)
 			khugepaged_scan.address = hstart;
 		if (khugepaged_scan.address > hend) {
 			khugepaged_scan.address = hend   HPAGE_PMD_SIZE;
 			progress  ;
 			continue;
 		}
 		BUG_ON(khugepaged_scan.address & ~HPAGE_PMD_MASK);
 
 		while (khugepaged_scan.address < hend) {
 			int ret;
 breakouterloop_mmap_sem:
 
 	spin_lock(&khugepaged_mm_lock);
 	BUG_ON(khugepaged_scan.mm_slot != mm_slot);
 	/*
 	 * Release the current mm_slot if this mm is about to die, or
 	 * if we scanned all vmas of this mm.
 
 	for (;;) {
 		mutex_unlock(&khugepaged_mutex);
 		BUG_ON(khugepaged_thread != current);
 		khugepaged_loop();
 		BUG_ON(khugepaged_thread != current);
 
 		mutex_lock(&khugepaged_mutex);
 		if (!khugepaged_enabled())
After patch : 
 	  (transparent_hugepage_flags &					\
 	   (1<<TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG) &&			\
 	   ((__vma) >vm_flags & VM_HUGEPAGE))) &&			\
 	 !((__vma) >vm_flags & VM_NOHUGEPAGE) &&			\
 	 !is_vma_temporary_stack(__vma))
 #define transparent_hugepage_defrag(__vma)				\
 	((transparent_hugepage_flags &					\
 	  (1<<TRANSPARENT_HUGEPAGE_DEFRAG_FLAG)) ||			\
 	/* VM_PFNMAP vmas may have vm_ops null but vm_file set */
 	if (!vma >anon_vma || vma >vm_ops || vma >vm_file)
 		goto out;
 	if (is_vma_temporary_stack(vma))
 		goto out;
 	VM_BUG_ON(is_linear_pfn_mapping(vma) || is_pfn_mapping(vma));
 
 	pgd = pgd_offset(mm, address);
 		if ((!(vma >vm_flags & VM_HUGEPAGE) &&
 		     !khugepaged_always()) ||
 		    (vma >vm_flags & VM_NOHUGEPAGE)) {
 		skip:
 			progress  ;
 			continue;
 		}
 		/* VM_PFNMAP vmas may have vm_ops null but vm_file set */
 		if (!vma >anon_vma || vma >vm_ops || vma >vm_file)
 			goto skip;
 		if (is_vma_temporary_stack(vma))
 			goto skip;
 
 		VM_BUG_ON(is_linear_pfn_mapping(vma) || is_pfn_mapping(vma));
 
 		hstart = (vma >vm_start   ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;
 		hend = vma >vm_end & HPAGE_PMD_MASK;
 		if (hstart >= hend)
 			goto skip;
 		if (khugepaged_scan.address > hend)
 			goto skip;
 		if (khugepaged_scan.address < hstart)
 			khugepaged_scan.address = hstart;
 		VM_BUG_ON(khugepaged_scan.address & ~HPAGE_PMD_MASK);
 
 		while (khugepaged_scan.address < hend) {
 			int ret;
 breakouterloop_mmap_sem:
 
 	spin_lock(&khugepaged_mm_lock);
 	VM_BUG_ON(khugepaged_scan.mm_slot != mm_slot);
 	/*
 	 * Release the current mm_slot if this mm is about to die, or
 	 * if we scanned all vmas of this mm.
 
 	for (;;) {
 		mutex_unlock(&khugepaged_mutex);
 		VM_BUG_ON(khugepaged_thread != current);
 		khugepaged_loop();
 		VM_BUG_ON(khugepaged_thread != current);
 
 		mutex_lock(&khugepaged_mutex);
 		if (!khugepaged_enabled())
