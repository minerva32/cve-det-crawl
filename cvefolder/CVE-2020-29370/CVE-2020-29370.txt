CVE Number : CVE-2020-29370
Commit Message : 
mm: slub: add missing TID bump in kmem_cache_alloc_bulk()
Commit Details : 
When kmem_cache_alloc_bulk() attempts to allocate N objects from a percpu
freelist of length M, and N > M > 0, it will first remove the M elements
from the percpu freelist, then call ___slab_alloc() to allocate the next
element and repopulate the percpu freelist. ___slab_alloc() can re-enable
IRQs via allocate_slab(), so the TID must be bumped before ___slab_alloc()
to properly commit the freelist head change.

Fix it by unconditionally bumping c->tid when entering the slowpath.

Cc: stable@vger.kernel.org
Fixes: ebe909e0fdb3 ("slub: improve bulk alloc strategy")
Signed-off-by: Jann Horn <jannh@google.com>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

Before patch : 
 
 		if (unlikely(!object)) {
 			/*
 			 * Invoking slow path likely have side effect
 			 * of re populating per CPU c >freelist
 			 */
After patch : 
 
 		if (unlikely(!object)) {
 			/*
 			 * We may have removed an object from c >freelist using
 			 * the fastpath in the previous iteration; in that case,
 			 * c >tid has not been bumped yet.
 			 * Since ___slab_alloc() may reenable interrupts while
 			 * allocating memory, we should bump c >tid now.
 			 */
 			c >tid = next_tid(c >tid);
 
 			/*
 			 * Invoking slow path likely have side effect
 			 * of re populating per CPU c >freelist
 			 */
