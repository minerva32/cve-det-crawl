CVE Number : CVE-2013-1797
Commit Message : 
KVM: x86: Convert MSR_KVM_SYSTEM_TIME to use gfn_to_hva_cache functions (CVE-2013-1797)
Commit Details : 
There is a potential use after free issue with the handling of
MSR_KVM_SYSTEM_TIME.  If the guest specifies a GPA in a movable or removable
memory such as frame buffers then KVM might continue to write to that
address even after it's removed via KVM_SET_USER_MEMORY_REGION.  KVM pins
the page in memory so it's unlikely to cause an issue, but if the user
space component re-purposes the memory previously used for the guest, then
the guest will be able to corrupt that memory.

Tested: Tested against kvmclock unit test

Signed-off-by: Andrew Honig <ahonig@google.com>
Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

Before patch : 
 	gpa_t time;
 	struct pvclock_vcpu_time_info hv_clock;
 	unsigned int hw_tsc_khz;
 	unsigned int time_offset;
 	struct page *time_page;
 	/* set guest stopped flag in pvclock flags field */
 	bool pvclock_set_guest_stopped_request;
 
 	unsigned long flags, this_tsc_khz;
 	struct kvm_vcpu_arch *vcpu = &v >arch;
 	struct kvm_arch *ka = &v >kvm >arch;
 	void *shared_kaddr;
 	s64 kernel_ns, max_kernel_ns;
 	u64 tsc_timestamp, host_tsc;
 	struct pvclock_vcpu_time_info *guest_hv_clock;
 	u8 pvclock_flags;
 	bool use_master_clock;
 
 
 	local_irq_restore(flags);
 
 	if (!vcpu >time_page)
 		return 0;
 
 	/*
 	 */
 	vcpu >hv_clock.version  = 2;
 
 	shared_kaddr = kmap_atomic(vcpu >time_page);
 
 	guest_hv_clock = shared_kaddr   vcpu >time_offset;
 
 	/* retain PVCLOCK_GUEST_STOPPED if set in guest copy */
 	pvclock_flags = (guest_hv_clock >flags & PVCLOCK_GUEST_STOPPED);
 
 	if (vcpu >pvclock_set_guest_stopped_request) {
 		pvclock_flags |= PVCLOCK_GUEST_STOPPED;
 
 	vcpu >hv_clock.flags = pvclock_flags;
 
 	memcpy(shared_kaddr   vcpu >time_offset, &vcpu >hv_clock,
 	       sizeof(vcpu >hv_clock));
 
 	kunmap_atomic(shared_kaddr);
 
 	mark_page_dirty(v >kvm, vcpu >time >> PAGE_SHIFT);
 	return 0;
 }
 
 
 static void kvmclock_reset(struct kvm_vcpu *vcpu)
 {
 	if (vcpu >arch.time_page) {
 		kvm_release_page_dirty(vcpu >arch.time_page);
 		vcpu >arch.time_page = NULL;
 	}
 }
 
 static void accumulate_steal_time(struct kvm_vcpu *vcpu)
 		break;
 	case MSR_KVM_SYSTEM_TIME_NEW:
 	case MSR_KVM_SYSTEM_TIME: {
 		kvmclock_reset(vcpu);
 
 		vcpu >arch.time = data;
 		if (!(data & 1))
 			break;
 
 		/* ...but clean it before doing the actual write */
 		vcpu >arch.time_offset = data & ~(PAGE_MASK | 1);
 
 		/* Check that the address is 32 byte aligned. */
 		if (vcpu >arch.time_offset &
 				(sizeof(struct pvclock_vcpu_time_info)   1))
 			break;
 
 		vcpu >arch.time_page =
 				gfn_to_page(vcpu >kvm, data >> PAGE_SHIFT);
 
 		if (is_error_page(vcpu >arch.time_page))
 			vcpu >arch.time_page = NULL;
 
 		break;
 	}
  */
 static int kvm_set_guest_paused(struct kvm_vcpu *vcpu)
 {
 	if (!vcpu >arch.time_page)
 		return  EINVAL;
 	vcpu >arch.pvclock_set_guest_stopped_request = true;
 	kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 		goto fail_free_wbinvd_dirty_mask;
 
 	vcpu >arch.ia32_tsc_adjust_msr = 0x0;
 	kvm_async_pf_hash_reset(vcpu);
 	kvm_pmu_init(vcpu);
 
After patch : 
 	gpa_t time;
 	struct pvclock_vcpu_time_info hv_clock;
 	unsigned int hw_tsc_khz;
 	struct gfn_to_hva_cache pv_time;
 	bool pv_time_enabled;
 	/* set guest stopped flag in pvclock flags field */
 	bool pvclock_set_guest_stopped_request;
 
 	unsigned long flags, this_tsc_khz;
 	struct kvm_vcpu_arch *vcpu = &v >arch;
 	struct kvm_arch *ka = &v >kvm >arch;
 	s64 kernel_ns, max_kernel_ns;
 	u64 tsc_timestamp, host_tsc;
 	struct pvclock_vcpu_time_info guest_hv_clock;
 	u8 pvclock_flags;
 	bool use_master_clock;
 
 
 	local_irq_restore(flags);
 
 	if (!vcpu >pv_time_enabled)
 		return 0;
 
 	/*
 	 */
 	vcpu >hv_clock.version  = 2;
 
 	if (unlikely(kvm_read_guest_cached(v >kvm, &vcpu >pv_time,
 		&guest_hv_clock, sizeof(guest_hv_clock))))
 		return 0;
 
 	/* retain PVCLOCK_GUEST_STOPPED if set in guest copy */
 	pvclock_flags = (guest_hv_clock.flags & PVCLOCK_GUEST_STOPPED);
 
 	if (vcpu >pvclock_set_guest_stopped_request) {
 		pvclock_flags |= PVCLOCK_GUEST_STOPPED;
 
 	vcpu >hv_clock.flags = pvclock_flags;
 
 	kvm_write_guest_cached(v >kvm, &vcpu >pv_time,
 				&vcpu >hv_clock,
 				sizeof(vcpu >hv_clock));
 	return 0;
 }
 
 
 static void kvmclock_reset(struct kvm_vcpu *vcpu)
 {
 	vcpu >arch.pv_time_enabled = false;
 }
 
 static void accumulate_steal_time(struct kvm_vcpu *vcpu)
 		break;
 	case MSR_KVM_SYSTEM_TIME_NEW:
 	case MSR_KVM_SYSTEM_TIME: {
 		u64 gpa_offset;
 		kvmclock_reset(vcpu);
 
 		vcpu >arch.time = data;
 		if (!(data & 1))
 			break;
 
 		gpa_offset = data & ~(PAGE_MASK | 1);
 
 		/* Check that the address is 32 byte aligned. */
 		if (gpa_offset & (sizeof(struct pvclock_vcpu_time_info)   1))
 			break;
 
 		if (kvm_gfn_to_hva_cache_init(vcpu >kvm,
 		     &vcpu >arch.pv_time, data & ~1ULL))
 			vcpu >arch.pv_time_enabled = false;
 		else
 			vcpu >arch.pv_time_enabled = true;
 
 		break;
 	}
  */
 static int kvm_set_guest_paused(struct kvm_vcpu *vcpu)
 {
 	if (!vcpu >arch.pv_time_enabled)
 		return  EINVAL;
 	vcpu >arch.pvclock_set_guest_stopped_request = true;
 	kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 		goto fail_free_wbinvd_dirty_mask;
 
 	vcpu >arch.ia32_tsc_adjust_msr = 0x0;
 	vcpu >arch.pv_time_enabled = false;
 	kvm_async_pf_hash_reset(vcpu);
 	kvm_pmu_init(vcpu);
 
