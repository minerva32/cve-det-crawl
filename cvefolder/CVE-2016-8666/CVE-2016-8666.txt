CVE Number : CVE-2016-8666
Commit Message : 
tunnels: Don't apply GRO to multiple layers of encapsulation.
Commit Details : 
When drivers express support for TSO of encapsulated packets, they
only mean that they can do it for one layer of encapsulation.
Supporting additional levels would mean updating, at a minimum,
more IP length fields and they are unaware of this.

No encapsulation device expresses support for handling offloaded
encapsulated packets, so we won't generate these types of frames
in the transmit path. However, GRO doesn't have a check for
multiple levels of encapsulation and will attempt to build them.

UDP tunnel GRO actually does prevent this situation but it only
handles multiple UDP tunnels stacked on top of each other. This
generalizes that solution to prevent any kind of tunnel stacking
that would cause problems.

Fixes: bf5a755f ("net-gre-gro: Add GRE support to the GRO stack")
Signed-off-by: Jesse Gross <jesse@kernel.org>
Signed-off-by: David S. Miller <davem@davemloft.net>

Before patch : 
 	/* This is non zero if the packet may be of the same flow. */
 	u8	same_flow:1;
 
 	/* Used in udp_gro_receive */
 	u8	udp_mark:1;
 
 	/* GRO checksum is valid */
 	u8	csum_valid:1;
 		NAPI_GRO_CB(skb) >same_flow = 0;
 		NAPI_GRO_CB(skb) >flush = 0;
 		NAPI_GRO_CB(skb) >free = 0;
 		NAPI_GRO_CB(skb) >udp_mark = 0;
 		NAPI_GRO_CB(skb) >gro_remcsum_start = 0;
 
 		/* Setup for GRO checksum validation */
 	return pp;
 }
 
 #define SECONDS_PER_DAY	86400
 
 /* inet_current_timestamp   Return IP network timestamp
 static const struct net_offload ipip_offload = {
 	.callbacks = {
 		.gso_segment	= inet_gso_segment,
 		.gro_receive	= inet_gro_receive,
 		.gro_complete	= ipip_gro_complete,
 	},
 };
 	struct packet_offload *ptype;
 	__be16 type;
 
 	off = skb_gro_offset(skb);
 	hlen = off   sizeof(*greh);
 	greh = skb_gro_header_fast(skb, off);
 	unsigned int off = skb_gro_offset(skb);
 	int flush = 1;
 
 	if (NAPI_GRO_CB(skb) >udp_mark ||
 	    (skb >ip_summed != CHECKSUM_PARTIAL &&
 	     NAPI_GRO_CB(skb) >csum_cnt == 0 &&
 	     !NAPI_GRO_CB(skb) >csum_valid))
 		goto out;
 
 	/* mark that this skb passed once through the udp gro layer */
 	NAPI_GRO_CB(skb) >udp_mark = 1;
 
 	rcu_read_lock();
 	uo_priv = rcu_dereference(udp_offload_base);
 	return pp;
 }
 
 static int ipv6_gro_complete(struct sk_buff *skb, int nhoff)
 {
 	const struct net_offload *ops;
 static const struct net_offload sit_offload = {
 	.callbacks = {
 		.gso_segment	= ipv6_gso_segment,
 		.gro_receive    = ipv6_gro_receive,
 		.gro_complete   = sit_gro_complete,
 	},
 };
After patch : 
 	/* This is non zero if the packet may be of the same flow. */
 	u8	same_flow:1;
 
 	/* Used in tunnel GRO receive */
 	u8	encap_mark:1;
 
 	/* GRO checksum is valid */
 	u8	csum_valid:1;
 		NAPI_GRO_CB(skb) >same_flow = 0;
 		NAPI_GRO_CB(skb) >flush = 0;
 		NAPI_GRO_CB(skb) >free = 0;
 		NAPI_GRO_CB(skb) >encap_mark = 0;
 		NAPI_GRO_CB(skb) >gro_remcsum_start = 0;
 
 		/* Setup for GRO checksum validation */
 	return pp;
 }
 
 static struct sk_buff **ipip_gro_receive(struct sk_buff **head,
 					 struct sk_buff *skb)
 {
 	if (NAPI_GRO_CB(skb) >encap_mark) {
 		NAPI_GRO_CB(skb) >flush = 1;
 		return NULL;
 	}
 
 	NAPI_GRO_CB(skb) >encap_mark = 1;
 
 	return inet_gro_receive(head, skb);
 }
 
 #define SECONDS_PER_DAY	86400
 
 /* inet_current_timestamp   Return IP network timestamp
 static const struct net_offload ipip_offload = {
 	.callbacks = {
 		.gso_segment	= inet_gso_segment,
 		.gro_receive	= ipip_gro_receive,
 		.gro_complete	= ipip_gro_complete,
 	},
 };
 	struct packet_offload *ptype;
 	__be16 type;
 
 	if (NAPI_GRO_CB(skb) >encap_mark)
 		goto out;
 
 	NAPI_GRO_CB(skb) >encap_mark = 1;
 
 	off = skb_gro_offset(skb);
 	hlen = off   sizeof(*greh);
 	greh = skb_gro_header_fast(skb, off);
 	unsigned int off = skb_gro_offset(skb);
 	int flush = 1;
 
 	if (NAPI_GRO_CB(skb) >encap_mark ||
 	    (skb >ip_summed != CHECKSUM_PARTIAL &&
 	     NAPI_GRO_CB(skb) >csum_cnt == 0 &&
 	     !NAPI_GRO_CB(skb) >csum_valid))
 		goto out;
 
 	/* mark that this skb passed once through the tunnel gro layer */
 	NAPI_GRO_CB(skb) >encap_mark = 1;
 
 	rcu_read_lock();
 	uo_priv = rcu_dereference(udp_offload_base);
 	return pp;
 }
 
 static struct sk_buff **sit_gro_receive(struct sk_buff **head,
 					struct sk_buff *skb)
 {
 	if (NAPI_GRO_CB(skb) >encap_mark) {
 		NAPI_GRO_CB(skb) >flush = 1;
 		return NULL;
 	}
 
 	NAPI_GRO_CB(skb) >encap_mark = 1;
 
 	return ipv6_gro_receive(head, skb);
 }
 
 static int ipv6_gro_complete(struct sk_buff *skb, int nhoff)
 {
 	const struct net_offload *ops;
 static const struct net_offload sit_offload = {
 	.callbacks = {
 		.gso_segment	= ipv6_gso_segment,
 		.gro_receive    = sit_gro_receive,
 		.gro_complete   = sit_gro_complete,
 	},
 };
