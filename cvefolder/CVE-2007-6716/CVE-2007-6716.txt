CVE Number : CVE-2007-6716
Commit Message : 
dio: zero struct dio with kzalloc instead of manually
Commit Details : 
This patch uses kzalloc to zero all of struct dio rather than manually
trying to track which fields we rely on being zero.  It passed aio+dio
stress testing and some bug regression testing on ext3.

This patch was introduced by Linus in the conversation that lead up to
Badari's minimal fix to manually zero .map_bh.b_state in commit:

  6a648fa72161d1f6468dabd96c5d3c0db04f598a

It makes the code a bit smaller.  Maybe a couple fewer cachelines to
load, if we're lucky:

   text    data     bss     dec     hex filename
3285925  568506 1304616 5159047  4eb887 vmlinux
3285797  568506 1304616 5158919  4eb807 vmlinux.patched

I was unable to measure a stable difference in the number of cpu cycles
spent in blockdev_direct_IO() when pushing aio+dio 256K reads at
~340MB/s.

So the resulting intent of the patch isn't a performance gain but to
avoid exposing ourselves to the risk of finding another field like
.map_bh.b_state where we rely on zeroing but don't enforce it in the
code.

Signed-off-by: Zach Brown <zach.brown@oracle.com>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

Before patch : 
 	ssize_t ret2;
 	size_t bytes;
 
 	dio >bio = NULL;
 	dio >inode = inode;
 	dio >rw = rw;
 	dio >blkbits = blkbits;
 	dio >blkfactor = inode >i_blkbits   blkbits;
 	dio >start_zero_done = 0;
 	dio >size = 0;
 	dio >block_in_file = offset >> blkbits;
 	dio >blocks_available = 0;
 	dio >cur_page = NULL;
 
 	dio >boundary = 0;
 	dio >reap_counter = 0;
 	dio >get_block = get_block;
 	dio >end_io = end_io;
 	dio >map_bh.b_private = NULL;
 	dio >map_bh.b_state = 0;
 	dio >final_block_in_bio =  1;
 	dio >next_block_for_io =  1;
 
 	dio >page_errors = 0;
 	dio >io_error = 0;
 	dio >result = 0;
 	dio >iocb = iocb;
 	dio >i_size = i_size_read(inode);
 
 	spin_lock_init(&dio >bio_lock);
 	dio >refcount = 1;
 	dio >bio_list = NULL;
 	dio >waiter = NULL;
 
 	/*
 	 * In case of non aligned buffers, we may need 2 more
 	 */
 	if (unlikely(dio >blkfactor))
 		dio >pages_in_io = 2;
 	else
 		dio >pages_in_io = 0;
 
 	for (seg = 0; seg < nr_segs; seg  ) {
 		user_addr = (unsigned long)iov[seg].iov_base;
 		}
 	}
 
 	dio = kmalloc(sizeof(*dio), GFP_KERNEL);
 	retval =  ENOMEM;
 	if (!dio)
 		goto out;
After patch : 
 	ssize_t ret2;
 	size_t bytes;
 
 	dio >inode = inode;
 	dio >rw = rw;
 	dio >blkbits = blkbits;
 	dio >blkfactor = inode >i_blkbits   blkbits;
 	dio >block_in_file = offset >> blkbits;
 
 	dio >get_block = get_block;
 	dio >end_io = end_io;
 	dio >final_block_in_bio =  1;
 	dio >next_block_for_io =  1;
 
 	dio >iocb = iocb;
 	dio >i_size = i_size_read(inode);
 
 	spin_lock_init(&dio >bio_lock);
 	dio >refcount = 1;
 
 	/*
 	 * In case of non aligned buffers, we may need 2 more
 	 */
 	if (unlikely(dio >blkfactor))
 		dio >pages_in_io = 2;
 
 	for (seg = 0; seg < nr_segs; seg  ) {
 		user_addr = (unsigned long)iov[seg].iov_base;
 		}
 	}
 
 	dio = kzalloc(sizeof(*dio), GFP_KERNEL);
 	retval =  ENOMEM;
 	if (!dio)
 		goto out;
