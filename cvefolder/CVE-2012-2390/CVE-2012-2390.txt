CVE Number : CVE-2012-2390
Commit Message : 
hugetlb: fix resv_map leak in error path
Commit Details : 
When called for anonymous (non-shared) mappings, hugetlb_reserve_pages()
does a resv_map_alloc().  It depends on code in hugetlbfs's
vm_ops->close() to release that allocation.

However, in the mmap() failure path, we do a plain unmap_region() without
the remove_vma() which actually calls vm_ops->close().

This is a decent fix.  This leak could get reintroduced if new code (say,
after hugetlb_reserve_pages() in hugetlbfs_file_mmap()) decides to return
an error.  But, I think it would have to unroll the reservation anyway.

Christoph's test case:

	http://marc.info/?l=linux-mm&m=133728900729735

This patch applies to 3.4 and later.  A version for earlier kernels is at
https://lkml.org/lkml/2012/5/22/418.

Signed-off-by: Dave Hansen <dave@linux.vnet.ibm.com>
Acked-by: Mel Gorman <mel@csn.ul.ie>
Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Reported-by: Christoph Lameter <cl@linux.com>
Tested-by: Christoph Lameter <cl@linux.com>
Cc: Andrea Arcangeli <aarcange@redhat.com>
Cc: <stable@vger.kernel.org>	2.6.32+
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

Before patch : 
 		kref_get(&reservations >refs);
 }
 
 static void hugetlb_vm_op_close(struct vm_area_struct *vma)
 {
 	struct hstate *h = hstate_vma(vma);
 		reserve = (end   start)  
 			region_count(&reservations >regions, start, end);
 
 		kref_put(&reservations >refs, resv_map_release);
 
 		if (reserve) {
 			hugetlb_acct_memory(h,  reserve);
 		set_vma_resv_flags(vma, HPAGE_RESV_OWNER);
 	}
 
 	if (chg < 0)
 		return chg;
 
 	/* There must be enough pages in the subpool for the mapping */
 	if (hugepage_subpool_get_pages(spool, chg))
 		return  ENOSPC;
 
 	/*
 	 * Check enough hugepages are available for the reservation.
 	ret = hugetlb_acct_memory(h, chg);
 	if (ret < 0) {
 		hugepage_subpool_put_pages(spool, chg);
 		return ret;
 	}
 
 	/*
 	if (!vma || vma >vm_flags & VM_MAYSHARE)
 		region_add(&inode >i_mapping >private_list, from, to);
 	return 0;
 }
 
 void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed)
After patch : 
 		kref_get(&reservations >refs);
 }
 
 static void resv_map_put(struct vm_area_struct *vma)
 {
 	struct resv_map *reservations = vma_resv_map(vma);
 
 	if (!reservations)
 		return;
 	kref_put(&reservations >refs, resv_map_release);
 }
 
 static void hugetlb_vm_op_close(struct vm_area_struct *vma)
 {
 	struct hstate *h = hstate_vma(vma);
 		reserve = (end   start)  
 			region_count(&reservations >regions, start, end);
 
 		resv_map_put(vma);
 
 		if (reserve) {
 			hugetlb_acct_memory(h,  reserve);
 		set_vma_resv_flags(vma, HPAGE_RESV_OWNER);
 	}
 
 	if (chg < 0) {
 		ret = chg;
 		goto out_err;
 	}
 
 	/* There must be enough pages in the subpool for the mapping */
 	if (hugepage_subpool_get_pages(spool, chg)) {
 		ret =  ENOSPC;
 		goto out_err;
 	}
 
 	/*
 	 * Check enough hugepages are available for the reservation.
 	ret = hugetlb_acct_memory(h, chg);
 	if (ret < 0) {
 		hugepage_subpool_put_pages(spool, chg);
 		goto out_err;
 	}
 
 	/*
 	if (!vma || vma >vm_flags & VM_MAYSHARE)
 		region_add(&inode >i_mapping >private_list, from, to);
 	return 0;
 out_err:
 	resv_map_put(vma);
 	return ret;
 }
 
 void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed)
