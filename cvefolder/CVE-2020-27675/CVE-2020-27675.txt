CVE Number : CVE-2020-27675
Commit Message : 
xen/events: avoid removing an event channel while handling it
Commit Details : 
Today it can happen that an event channel is being removed from the
system while the event handling loop is active. This can lead to a
race resulting in crashes or WARN() splats when trying to access the
irq_info structure related to the event channel.

Fix this problem by using a rwlock taken as reader in the event
handling loop and as writer when deallocating the irq_info structure.

As the observed problem was a NULL dereference in evtchn_from_irq()
make this function more robust against races by testing the irq_info
pointer to be not NULL before dereferencing it.

And finally make all accesses to evtchn_to_irqrowcol atomic ones
in order to avoid seeing partial updates of an array element in irq
handling. Note that irq handling can be entered only for event channels
which have been valid before, so any not populated row isn't a problem
in this regard, as rows are only ever added and never removed.

This is XSA-331.

Cc: stable@vger.kernel.org
Reported-by: Marek Marczykowski-GÃ³recki <marmarek@invisiblethingslab.com>
Reported-by: Jinoh Kang <luke1337@theori.io>
Signed-off-by: Juergen Gross <jgross@suse.com>
Reviewed-by: Stefano Stabellini <sstabellini@kernel.org>
Reviewed-by: Wei Liu <wl@xen.org>

Before patch : 
 #include <linux/slab.h>
 #include <linux/irqnr.h>
 #include <linux/pci.h>
 
 #ifdef CONFIG_X86
 #include <asm/desc.h>
  */
 static DEFINE_MUTEX(irq_mapping_update_lock);
 
 static LIST_HEAD(xen_irq_list_head);
 
 /* IRQ < > VIRQ mapping. */
 	unsigned col;
 
 	for (col = 0; col < EVTCHN_PER_ROW; col  )
 		evtchn_to_irq[row][col] =  1;
 }
 
 static void clear_evtchn_to_irq_all(void)
 		clear_evtchn_to_irq_row(row);
 	}
 
 	evtchn_to_irq[row][col] = irq;
 	return 0;
 }
 
 		return  1;
 	if (evtchn_to_irq[EVTCHN_ROW(evtchn)] == NULL)
 		return  1;
 	return evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)];
 }
 
 /* Get info for IRQ */
  */
 evtchn_port_t evtchn_from_irq(unsigned irq)
 {
 	if (WARN(irq >= nr_irqs, "Invalid irq %d!\n", irq))
 		return 0;
 
 	return info_for_irq(irq) >evtchn;
 }
 
 unsigned int irq_from_evtchn(evtchn_port_t evtchn)
 static void xen_free_irq(unsigned irq)
 {
 	struct irq_info *info = info_for_irq(irq);
 
 	if (WARN_ON(!info))
 		return;
 
 	list_del(&info >list);
 
 	set_info_for_irq(irq, NULL);
 
 	WARN_ON(info >refcnt > 0);
 
 	kfree(info);
 
 	/* Legacy IRQ descriptors are managed by the arch. */
 	struct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);
 	int cpu = smp_processor_id();
 
 	do {
 		vcpu_info >evtchn_upcall_pending = 0;
 
 		virt_rmb(); /* Hypervisor can set upcall pending. */
 
 	} while (vcpu_info >evtchn_upcall_pending);
 }
 
 void xen_evtchn_do_upcall(struct pt_regs *regs)
After patch : 
 #include <linux/slab.h>
 #include <linux/irqnr.h>
 #include <linux/pci.h>
 #include <linux/spinlock.h>
 
 #ifdef CONFIG_X86
 #include <asm/desc.h>
  */
 static DEFINE_MUTEX(irq_mapping_update_lock);
 
 /*
  * Lock protecting event handling loop against removing event channels.
  * Adding of event channels is no issue as the associated IRQ becomes active
  * only after everything is setup (before request_[threaded_]irq() the handler
  * can't be entered for an event, as the event channel will be unmasked only
  * then).
  */
 static DEFINE_RWLOCK(evtchn_rwlock);
 
 /*
  * Lock hierarchy:
  *
  * irq_mapping_update_lock
  *   evtchn_rwlock
  *     IRQ desc lock
  */
 
 static LIST_HEAD(xen_irq_list_head);
 
 /* IRQ < > VIRQ mapping. */
 	unsigned col;
 
 	for (col = 0; col < EVTCHN_PER_ROW; col  )
 		WRITE_ONCE(evtchn_to_irq[row][col],  1);
 }
 
 static void clear_evtchn_to_irq_all(void)
 		clear_evtchn_to_irq_row(row);
 	}
 
 	WRITE_ONCE(evtchn_to_irq[row][col], irq);
 	return 0;
 }
 
 		return  1;
 	if (evtchn_to_irq[EVTCHN_ROW(evtchn)] == NULL)
 		return  1;
 	return READ_ONCE(evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)]);
 }
 
 /* Get info for IRQ */
  */
 evtchn_port_t evtchn_from_irq(unsigned irq)
 {
 	const struct irq_info *info = NULL;
 
 	if (likely(irq < nr_irqs))
 		info = info_for_irq(irq);
 	if (!info)
 		return 0;
 
 	return info >evtchn;
 }
 
 unsigned int irq_from_evtchn(evtchn_port_t evtchn)
 static void xen_free_irq(unsigned irq)
 {
 	struct irq_info *info = info_for_irq(irq);
 	unsigned long flags;
 
 	if (WARN_ON(!info))
 		return;
 
 	write_lock_irqsave(&evtchn_rwlock, flags);
 
 	list_del(&info >list);
 
 	set_info_for_irq(irq, NULL);
 
 	WARN_ON(info >refcnt > 0);
 
 	write_unlock_irqrestore(&evtchn_rwlock, flags);
 
 	kfree(info);
 
 	/* Legacy IRQ descriptors are managed by the arch. */
 	struct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);
 	int cpu = smp_processor_id();
 
 	read_lock(&evtchn_rwlock);
 
 	do {
 		vcpu_info >evtchn_upcall_pending = 0;
 
 		virt_rmb(); /* Hypervisor can set upcall pending. */
 
 	} while (vcpu_info >evtchn_upcall_pending);
 
 	read_unlock(&evtchn_rwlock);
 }
 
 void xen_evtchn_do_upcall(struct pt_regs *regs)
