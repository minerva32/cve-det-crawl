CVE Number : CVE-2011-2918
Commit Message : 
perf: Remove the nmi parameter from the swevent and overflow interface
Commit Details : 
The nmi parameter indicated if we could do wakeups from the current
context, if not, we would set some state and self-IPI and let the
resulting interrupt do the wakeup.

For the various event classes:

  - hardware: nmi=0; PMI is in fact an NMI or we run irq_work_run from
    the PMI-tail (ARM etc.)
  - tracepoint: nmi=0; since tracepoint could be from NMI context.
  - software: nmi=0,1; some, like the schedule thing cannot
    perform wakeups, and hence need 0.

As one can see, there is very little nmi=1 usage, and the down-side of
not using it is that on some platforms some software events can have a
jiffy delay in wakeup (when arch_irq_work_raise isn't implemented).

The up-side however is that we can remove the nmi parameter and save a
bunch of conditionals in fast paths.

Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
Cc: Michael Cree <mcree@orcon.net.nz>
Cc: Will Deacon <will.deacon@arm.com>
Cc: Deng-Cheng Zhu <dengcheng.zhu@gmail.com>
Cc: Anton Blanchard <anton@samba.org>
Cc: Eric B Munson <emunson@mgebm.net>
Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
Cc: Paul Mundt <lethal@linux-sh.org>
Cc: David S. Miller <davem@davemloft.net>
Cc: Frederic Weisbecker <fweisbec@gmail.com>
Cc: Jason Wessel <jason.wessel@windriver.com>
Cc: Don Zickus <dzickus@redhat.com>
Link: http://lkml.kernel.org/n/tip-agjev8eu666tvknpb3iaj0fg@git.kernel.org
Signed-off-by: Ingo Molnar <mingo@elte.hu>

Before patch : 
 	data.period = event >hw.last_period;
 
 	if (alpha_perf_event_set_period(event, hwc, idx)) {
 		if (perf_event_overflow(event, 1, &data, regs)) {
 			/* Interrupts coming too quickly; "throttle" the
 			 * counter, i.e., disable it for a little while.
 			 */
 		if (!armpmu_event_set_period(event, hwc, idx))
 			continue;
 
 		if (perf_event_overflow(event, 0, &data, regs))
 			armpmu >disable(hwc, idx);
 	}
 
 		if (!armpmu_event_set_period(event, hwc, idx))
 			continue;
 
 		if (perf_event_overflow(event, 0, &data, regs))
 			armpmu >disable(hwc, idx);
 	}
 
 		if (!armpmu_event_set_period(event, hwc, idx))
 			continue;
 
 		if (perf_event_overflow(event, 0, &data, regs))
 			armpmu >disable(hwc, idx);
 	}
 
 		if (!armpmu_event_set_period(event, hwc, idx))
 			continue;
 
 		if (perf_event_overflow(event, 0, &data, regs))
 			armpmu >disable(hwc, idx);
 	}
 
 /*
  * Handle hitting a HW breakpoint.
  */
 static void ptrace_hbptriggered(struct perf_event *bp, int unused,
 				     struct perf_sample_data *data,
 				     struct pt_regs *regs)
 {
 	unsigned int address, destreg, data, type;
 	unsigned int res = 0;
 
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, regs >ARM_pc);
 
 	if (current >pid != previous_pid) {
 		pr_debug("\"%s\" (%ld) uses deprecated SWP{B} instruction\n",
 	fault = __do_page_fault(mm, addr, fsr, tsk);
 	up_read(&mm >mmap_sem);
 
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, addr);
 	if (fault & VM_FAULT_MAJOR)
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0, regs, addr);
 	else if (fault & VM_FAULT_MINOR)
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0, regs, addr);
 
 	/*
 	 * Handle the "normal" case first   VM_FAULT_MAJOR / VM_FAULT_MINOR
 	if (!mipspmu_event_set_period(event, hwc, idx))
 		return;
 
 	if (perf_event_overflow(event, 0, data, regs))
 		mipspmu >disable_event(idx);
 }
 
 {
 	if ((opcode & OPCODE) == LL) {
 		perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
 				1, 0, regs, 0);
 		return simulate_ll(regs, opcode);
 	}
 	if ((opcode & OPCODE) == SC) {
 		perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
 				1, 0, regs, 0);
 		return simulate_sc(regs, opcode);
 	}
 
 		int rd = (opcode & RD) >> 11;
 		int rt = (opcode & RT) >> 16;
 		perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
 				1, 0, regs, 0);
 		switch (rd) {
 		case 0:		/* CPU number */
 			regs >regs[rt] = smp_processor_id();
 {
 	if ((opcode & OPCODE) == SPEC0 && (opcode & FUNC) == SYNC) {
 		perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
 				1, 0, regs, 0);
 		return 0;
 	}
 
 	unsigned long value;
 	unsigned int res;
 
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
 		      1, 0, regs, 0);
 
 	/*
 	 * This load never faults.
 	mm_segment_t seg;
 
 	perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS,
 			1, 0, regs, regs >cp0_badvaddr);
 	/*
 	 * Did we catch a fault trying to load an instruction?
 	 * Or are we running in MIPS16 mode?
 	}
 
       emul:
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
 			1, 0, xcp, 0);
 	MIPS_FPU_EMU_INC_STATS(emulated);
 	switch (MIPSInst_OPCODE(ir)) {
 	case ldc1_op:{
 	 * the fault.
 	 */
 	fault = handle_mm_fault(mm, vma, address, write ? FAULT_FLAG_WRITE : 0);
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
 	if (unlikely(fault & VM_FAULT_ERROR)) {
 		if (fault & VM_FAULT_OOM)
 			goto out_of_memory;
 		BUG();
 	}
 	if (fault & VM_FAULT_MAJOR) {
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ,
 				1, 0, regs, address);
 		tsk >maj_flt  ;
 	} else {
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN,
 				1, 0, regs, address);
 		tsk >min_flt  ;
 	}
 
 #define PPC_WARN_EMULATED(type, regs)					\
 	do {								\
 		perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,		\
 			1, 0, regs, 0);					\
 		__PPC_WARN_EMULATED(type);				\
 	} while (0)
 
 #define PPC_WARN_ALIGNMENT(type, regs)					\
 	do {								\
 		perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS,		\
 			1, 0, regs, regs >dar);				\
 		__PPC_WARN_EMULATED(type);				\
 	} while (0)
 
  * here so there is no possibility of being interrupted.
  */
 static void record_and_restart(struct perf_event *event, unsigned long val,
 			       struct pt_regs *regs, int nmi)
 {
 	u64 period = event >hw.sample_period;
 	s64 prev, delta, left;
 		if (event >attr.sample_type & PERF_SAMPLE_ADDR)
 			perf_get_data_addr(regs, &data.addr);
 
 		if (perf_event_overflow(event, nmi, &data, regs))
 			power_pmu_stop(event, 0);
 	}
 }
 		if ((int)val < 0) {
 			/* event has overflowed */
 			found = 1;
 			record_and_restart(event, val, regs, nmi);
 		}
 	}
 
  * here so there is no possibility of being interrupted.
  */
 static void record_and_restart(struct perf_event *event, unsigned long val,
 			       struct pt_regs *regs, int nmi)
 {
 	u64 period = event >hw.sample_period;
 	s64 prev, delta, left;
 		perf_sample_data_init(&data, 0);
 		data.period = event >hw.last_period;
 
 		if (perf_event_overflow(event, nmi, &data, regs))
 			fsl_emb_pmu_stop(event, 0);
 	}
 }
 			if (event) {
 				/* event has overflowed */
 				found = 1;
 				record_and_restart(event, val, regs, nmi);
 			} else {
 				/*
 				 * Disabled counter is negative,
 }
 
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
 void ptrace_triggered(struct perf_event *bp, int nmi,
 		      struct perf_sample_data *data, struct pt_regs *regs)
 {
 	struct perf_event_attr attr;
 		die("Weird page fault", regs, SIGSEGV);
 	}
 
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
 
 	/* When running in the kernel we expect faults to occur only to
 	 * addresses in user space.  All other faults represent errors in the
 	}
 	if (ret & VM_FAULT_MAJOR) {
 		current >maj_flt  ;
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
 				     regs, address);
 #ifdef CONFIG_PPC_SMLPAR
 		if (firmware_has_feature(FW_FEATURE_CMO)) {
 #endif
 	} else {
 		current >min_flt  ;
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
 				     regs, address);
 	}
 	up_read(&mm >mmap_sem);
 		goto out;
 
 	address = trans_exc_code & __FAIL_ADDR_MASK;
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
 	flags = FAULT_FLAG_ALLOW_RETRY;
 	if (access == VM_WRITE || (trans_exc_code & store_indication) == 0x400)
 		flags |= FAULT_FLAG_WRITE;
 	if (flags & FAULT_FLAG_ALLOW_RETRY) {
 		if (fault & VM_FAULT_MAJOR) {
 			tsk >maj_flt  ;
 			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
 				      regs, address);
 		} else {
 			tsk >min_flt  ;
 			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
 				      regs, address);
 		}
 		if (fault & VM_FAULT_RETRY) {
 	return 0;
 }
 
 void ptrace_triggered(struct perf_event *bp, int nmi,
 		      struct perf_sample_data *data, struct pt_regs *regs)
 {
 	struct perf_event_attr attr;
 	 */
 	if (!expected) {
 		unaligned_fixups_notify(current, instruction, regs);
 		perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0,
 			      regs, address);
 	}
 
 		return error;
 	}
 
 	perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, address);
 
 	destreg = (opcode >> 4) & 0x3f;
 	if (user_mode(regs)) {
 		return error;
 	}
 
 	perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, address);
 
 	srcreg = (opcode >> 4) & 0x3f;
 	if (user_mode(regs)) {
 		return error;
 	}
 
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, address);
 
 	destreg = (opcode >> 4) & 0x3f;
 	if (user_mode(regs)) {
 		return error;
 	}
 
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, address);
 
 	srcreg = (opcode >> 4) & 0x3f;
 	if (user_mode(regs)) {
 	struct task_struct *tsk = current;
 	struct sh_fpu_soft_struct *fpu = &(tsk >thread.xstate >softfpu);
 
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);
 
 	if (!(task_thread_info(tsk) >status & TS_USEDFPU)) {
 		/* initialize once. */
 	if ((regs >sr & SR_IMASK) != SR_IMASK)
 		local_irq_enable();
 
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
 
 	/*
 	 * If we're in an interrupt, have no user context or are running
 	}
 	if (fault & VM_FAULT_MAJOR) {
 		tsk >maj_flt  ;
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
 				     regs, address);
 	} else {
 		tsk >min_flt  ;
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
 				     regs, address);
 	}
 
 	/* Not an IO address, so reenable interrupts */
 	local_irq_enable();
 
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
 
 	/*
 	 * If we're in an interrupt or have no user
 
 	if (fault & VM_FAULT_MAJOR) {
 		tsk >maj_flt  ;
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
 				     regs, address);
 	} else {
 		tsk >min_flt  ;
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
 				     regs, address);
 	}
 
 		if (!sparc_perf_event_set_period(event, hwc, idx))
 			continue;
 
 		if (perf_event_overflow(event, 1, &data, regs))
 			sparc_pmu_stop(event, 0);
 	}
 
 		unsigned long addr = compute_effective_address(regs, insn);
 		int err;
 
 		perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, addr);
 		switch (dir) {
 		case load:
 			err = do_int_load(fetch_reg_addr(((insn>>25)&0x1f),
 		}
 
 		addr = compute_effective_address(regs, insn);
 		perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, addr);
 		switch(dir) {
 		case load:
 			err = do_int_load(fetch_reg_addr(((insn>>25)&0x1f),
 
 		addr = compute_effective_address(regs, insn,
 						 ((insn >> 25) & 0x1f));
 		perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, addr);
 		switch (asi) {
 		case ASI_NL:
 		case ASI_AIUPL:
 	int ret, i, rd = ((insn >> 25) & 0x1f);
 	int from_kernel = (regs >tstate & TSTATE_PRIV) != 0;
 
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);
 	if (insn & 0x2000) {
 		maybe_flush_windows(0, 0, rd, from_kernel);
 		value = sign_extend_imm13(insn);
 	int asi = decode_asi(insn, regs);
 	int flag = (freg < 32) ? FPRS_DL : FPRS_DU;
 
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);
 
 	save_and_clear_fpu();
 	current_thread_info() >xfsr[0] &= ~0x1c000;
 	int from_kernel = (regs >tstate & TSTATE_PRIV) != 0;
 	unsigned long *reg;
 
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);
 
 	maybe_flush_windows(0, 0, rd, from_kernel);
 	reg = fetch_reg_addr(rd, regs);
 
 	if (tstate & TSTATE_PRIV)
 		die_if_kernel("lddfmna from kernel", regs);
 	perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, sfar);
 	if (test_thread_flag(TIF_32BIT))
 		pc = (u32)pc;
 	if (get_user(insn, (u32 __user *) pc) !=  EFAULT) {
 
 	if (tstate & TSTATE_PRIV)
 		die_if_kernel("stdfmna from kernel", regs);
 	perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, sfar);
 	if (test_thread_flag(TIF_32BIT))
 		pc = (u32)pc;
 	if (get_user(insn, (u32 __user *) pc) !=  EFAULT) {
 
 	BUG_ON(regs >tstate & TSTATE_PRIV);
 
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);
 
 	if (test_thread_flag(TIF_32BIT))
 		pc = (u32)pc;
 	int retcode = 0;                               /* assume all succeed */
 	unsigned long insn;
 
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);
 
 #ifdef DEBUG_MATHEMU
 	printk("In do_mathemu()... pc is %08lx\n", regs >pc);
 
 	if (tstate & TSTATE_PRIV)
 		die_if_kernel("unfinished/unimplemented FPop from kernel", regs);
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);
 	if (test_thread_flag(TIF_32BIT))
 		pc = (u32)pc;
 	if (get_user(insn, (u32 __user *) pc) !=  EFAULT) {
         if (in_atomic() || !mm)
                 goto no_context;
 
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
 
 	down_read(&mm >mmap_sem);
 
 	}
 	if (fault & VM_FAULT_MAJOR) {
 		current >maj_flt  ;
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
 			      regs, address);
 	} else {
 		current >min_flt  ;
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
 			      regs, address);
 	}
 	up_read(&mm >mmap_sem);
 	return;
 	if (in_atomic() || !mm)
 		goto intr_or_no_mm;
 
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
 
 	if (!down_read_trylock(&mm >mmap_sem)) {
 		if ((regs >tstate & TSTATE_PRIV) &&
 	}
 	if (fault & VM_FAULT_MAJOR) {
 		current >maj_flt  ;
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
 			      regs, address);
 	} else {
 		current >min_flt  ;
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
 			      regs, address);
 	}
 	up_read(&mm >mmap_sem);
 
 		if (!x86_perf_event_set_period(event))
 			continue;
 
 		if (perf_event_overflow(event, 1, &data, regs))
 			x86_pmu_stop(event, 0);
 	}
 
 
 		data.period = event >hw.last_period;
 
 		if (perf_event_overflow(event, 1, &data, regs))
 			x86_pmu_stop(event, 0);
 	}
 
 	 */
 	perf_prepare_sample(&header, &data, event, &regs);
 
 	if (perf_output_begin(&handle, event, header.size * (top   at), 1, 1))
 		return 1;
 
 	for (; at < top; at  ) {
 	else
 		regs.flags &= ~PERF_EFLAGS_EXACT;
 
 	if (perf_event_overflow(event, 1, &data, &regs))
 		x86_pmu_stop(event, 0);
 }
 
 
 		if (!x86_perf_event_set_period(event))
 			continue;
 		if (perf_event_overflow(event, 1, &data, regs))
 			x86_pmu_stop(event, 0);
 	}
 
 	return register_die_notifier(&kgdb_notifier);
 }
 
 static void kgdb_hw_overflow_handler(struct perf_event *event, int nmi,
 		struct perf_sample_data *data, struct pt_regs *regs)
 {
 	struct task_struct *tsk = current;
 	return ret;
 }
 
 static void ptrace_triggered(struct perf_event *bp, int nmi,
 			     struct perf_sample_data *data,
 			     struct pt_regs *regs)
 {
 	if (unlikely(error_code & PF_RSVD))
 		pgtable_bad(regs, error_code, address);
 
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
 
 	/*
 	 * If we're in an interrupt, have no user context or are running
 	if (flags & FAULT_FLAG_ALLOW_RETRY) {
 		if (fault & VM_FAULT_MAJOR) {
 			tsk >maj_flt  ;
 			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
 				      regs, address);
 		} else {
 			tsk >min_flt  ;
 			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
 				      regs, address);
 		}
 		if (fault & VM_FAULT_RETRY) {
 struct file;
 struct perf_sample_data;
 
 typedef void (*perf_overflow_handler_t)(struct perf_event *, int,
 					struct perf_sample_data *,
 					struct pt_regs *regs);
 
 	unsigned long			size;
 	void				*addr;
 	int				page;
 	int				nmi;
 	int				sample;
 };
 
 				struct perf_event *event,
 				struct pt_regs *regs);
 
 extern int perf_event_overflow(struct perf_event *event, int nmi,
 				 struct perf_sample_data *data,
 				 struct pt_regs *regs);
 
 
 extern struct jump_label_key perf_swevent_enabled[PERF_COUNT_SW_MAX];
 
 extern void __perf_sw_event(u32, u64, int, struct pt_regs *, u64);
 
 #ifndef perf_arch_fetch_caller_regs
 static inline void perf_arch_fetch_caller_regs(struct pt_regs *regs, unsigned long ip) { }
 }
 
 static __always_inline void
 perf_sw_event(u32 event_id, u64 nr, int nmi, struct pt_regs *regs, u64 addr)
 {
 	struct pt_regs hot_regs;
 
 			perf_fetch_caller_regs(&hot_regs);
 			regs = &hot_regs;
 		}
 		__perf_sw_event(event_id, nr, nmi, regs, addr);
 	}
 }
 
 
 static inline void perf_event_task_sched_out(struct task_struct *task, struct task_struct *next)
 {
 	perf_sw_event(PERF_COUNT_SW_CONTEXT_SWITCHES, 1, 1, NULL, 0);
 
 	__perf_event_task_sched_out(task, next);
 }
 
 extern int perf_output_begin(struct perf_output_handle *handle,
 			     struct perf_event *event, unsigned int size,
 			     int nmi, int sample);
 extern void perf_output_end(struct perf_output_handle *handle);
 extern void perf_output_copy(struct perf_output_handle *handle,
 			     const void *buf, unsigned int len);
 static inline int perf_event_task_enable(void)				{ return  EINVAL; }
 
 static inline void
 perf_sw_event(u32 event_id, u64 nr, int nmi,
 		     struct pt_regs *regs, u64 addr)			{ }
 static inline void
 perf_bp_event(struct perf_event *event, void *data)			{ }
 
 	}
 }
 
 static void perf_event_output(struct perf_event *event, int nmi,
 				struct perf_sample_data *data,
 				struct pt_regs *regs)
 {
 
 	perf_prepare_sample(&header, data, event, regs);
 
 	if (perf_output_begin(&handle, event, header.size, nmi, 1))
 		goto exit;
 
 	perf_output_sample(&handle, &header, data, event);
 	int ret;
 
 	perf_event_header__init_id(&read_event.header, &sample, event);
 	ret = perf_output_begin(&handle, event, read_event.header.size, 0, 0);
 	if (ret)
 		return;
 
 	perf_event_header__init_id(&task_event >event_id.header, &sample, event);
 
 	ret = perf_output_begin(&handle, event,
 				task_event >event_id.header.size, 0, 0);
 	if (ret)
 		goto out;
 
 
 	perf_event_header__init_id(&comm_event >event_id.header, &sample, event);
 	ret = perf_output_begin(&handle, event,
 				comm_event >event_id.header.size, 0, 0);
 
 	if (ret)
 		goto out;
 
 	perf_event_header__init_id(&mmap_event >event_id.header, &sample, event);
 	ret = perf_output_begin(&handle, event,
 				mmap_event >event_id.header.size, 0, 0);
 	if (ret)
 		goto out;
 
 	perf_event_header__init_id(&throttle_event.header, &sample, event);
 
 	ret = perf_output_begin(&handle, event,
 				throttle_event.header.size, 1, 0);
 	if (ret)
 		return;
 
  * Generic event overflow handling, sampling.
  */
 
 static int __perf_event_overflow(struct perf_event *event, int nmi,
 				   int throttle, struct perf_sample_data *data,
 				   struct pt_regs *regs)
 {
 	if (events && atomic_dec_and_test(&event >event_limit)) {
 		ret = 1;
 		event >pending_kill = POLL_HUP;
 		if (nmi) {
 			event >pending_disable = 1;
 			irq_work_queue(&event >pending);
 		} else
 			perf_event_disable(event);
 	}
 
 	if (event >overflow_handler)
 		event >overflow_handler(event, nmi, data, regs);
 	else
 		perf_event_output(event, nmi, data, regs);
 
 	if (event >fasync && event >pending_kill) {
 		if (nmi) {
 			event >pending_wakeup = 1;
 			irq_work_queue(&event >pending);
 		} else
 			perf_event_wakeup(event);
 	}
 
 	return ret;
 }
 
 int perf_event_overflow(struct perf_event *event, int nmi,
 			  struct perf_sample_data *data,
 			  struct pt_regs *regs)
 {
 	return __perf_event_overflow(event, nmi, 1, data, regs);
 }
 
 /*
 }
 
 static void perf_swevent_overflow(struct perf_event *event, u64 overflow,
 				    int nmi, struct perf_sample_data *data,
 				    struct pt_regs *regs)
 {
 	struct hw_perf_event *hwc = &event >hw;
 		return;
 
 	for (; overflow; overflow  ) {
 		if (__perf_event_overflow(event, nmi, throttle,
 					    data, regs)) {
 			/*
 			 * We inhibit the overflow from happening when
 }
 
 static void perf_swevent_event(struct perf_event *event, u64 nr,
 			       int nmi, struct perf_sample_data *data,
 			       struct pt_regs *regs)
 {
 	struct hw_perf_event *hwc = &event >hw;
 		return;
 
 	if (nr == 1 && hwc >sample_period == 1 && !event >attr.freq)
 		return perf_swevent_overflow(event, 1, nmi, data, regs);
 
 	if (local64_add_negative(nr, &hwc >period_left))
 		return;
 
 	perf_swevent_overflow(event, 0, nmi, data, regs);
 }
 
 static int perf_exclude_event(struct perf_event *event,
 }
 
 static void do_perf_sw_event(enum perf_type_id type, u32 event_id,
 				    u64 nr, int nmi,
 				    struct perf_sample_data *data,
 				    struct pt_regs *regs)
 {
 
 	hlist_for_each_entry_rcu(event, node, head, hlist_entry) {
 		if (perf_swevent_match(event, type, event_id, data, regs))
 			perf_swevent_event(event, nr, nmi, data, regs);
 	}
 end:
 	rcu_read_unlock();
 	put_recursion_context(swhash >recursion, rctx);
 }
 
 void __perf_sw_event(u32 event_id, u64 nr, int nmi,
 			    struct pt_regs *regs, u64 addr)
 {
 	struct perf_sample_data data;
 	int rctx;
 
 	perf_sample_data_init(&data, addr);
 
 	do_perf_sw_event(PERF_TYPE_SOFTWARE, event_id, nr, nmi, &data, regs);
 
 	perf_swevent_put_recursion_context(rctx);
 	preempt_enable_notrace();
 
 	hlist_for_each_entry_rcu(event, node, head, hlist_entry) {
 		if (perf_tp_event_match(event, &data, regs))
 			perf_swevent_event(event, count, 1, &data, regs);
 	}
 
 	perf_swevent_put_recursion_context(rctx);
 	perf_sample_data_init(&sample, bp >attr.bp_addr);
 
 	if (!bp >hw.state && !perf_exclude_event(bp, regs))
 		perf_swevent_event(bp, 1, 1, &sample, regs);
 }
 #endif
 
 
 	if (regs && !perf_exclude_event(event, regs)) {
 		if (!(event >attr.exclude_idle && current >pid == 0))
 			if (perf_event_overflow(event, 0, &data, regs))
 				ret = HRTIMER_NORESTART;
 	}
 
 	void				*data_pages[0];
 };
 
 
 extern void rb_free(struct ring_buffer *rb);
 extern struct ring_buffer *
 rb_alloc(int nr_pages, long watermark, int cpu, int flags);
 {
 	atomic_set(&handle >rb >poll, POLL_IN);
 
 	if (handle >nmi) {
 		handle >event >pending_wakeup = 1;
 		irq_work_queue(&handle >event >pending);
 	} else
 		perf_event_wakeup(handle >event);
 }
 
 /*
 
 int perf_output_begin(struct perf_output_handle *handle,
 		      struct perf_event *event, unsigned int size,
 		      int nmi, int sample)
 {
 	struct ring_buffer *rb;
 	unsigned long tail, offset, head;
 
 	handle >rb	= rb;
 	handle >event	= event;
 	handle >nmi	= nmi;
 	handle >sample	= sample;
 
 	if (!rb >nr_pages)
 
 	if (task_cpu(p) != new_cpu) {
 		p >se.nr_migrations  ;
 		perf_sw_event(PERF_COUNT_SW_CPU_MIGRATIONS, 1, 1, NULL, 0);
 	}
 
 	__set_task_cpu(p, new_cpu);
 };
 
 /* Callback function for perf event subsystem */
 static void watchdog_overflow_callback(struct perf_event *event, int nmi,
 		 struct perf_sample_data *data,
 		 struct pt_regs *regs)
 {
 MODULE_PARM_DESC(ksym, "Kernel symbol to monitor; this module will report any"
 			" write operations on the kernel symbol");
 
 static void sample_hbp_handler(struct perf_event *bp, int nmi,
 			       struct perf_sample_data *data,
 			       struct pt_regs *regs)
 {
After patch : 
 	data.period = event >hw.last_period;
 
 	if (alpha_perf_event_set_period(event, hwc, idx)) {
 		if (perf_event_overflow(event, &data, regs)) {
 			/* Interrupts coming too quickly; "throttle" the
 			 * counter, i.e., disable it for a little while.
 			 */
 		if (!armpmu_event_set_period(event, hwc, idx))
 			continue;
 
 		if (perf_event_overflow(event, &data, regs))
 			armpmu >disable(hwc, idx);
 	}
 
 		if (!armpmu_event_set_period(event, hwc, idx))
 			continue;
 
 		if (perf_event_overflow(event, &data, regs))
 			armpmu >disable(hwc, idx);
 	}
 
 		if (!armpmu_event_set_period(event, hwc, idx))
 			continue;
 
 		if (perf_event_overflow(event, &data, regs))
 			armpmu >disable(hwc, idx);
 	}
 
 		if (!armpmu_event_set_period(event, hwc, idx))
 			continue;
 
 		if (perf_event_overflow(event, &data, regs))
 			armpmu >disable(hwc, idx);
 	}
 
 /*
  * Handle hitting a HW breakpoint.
  */
 static void ptrace_hbptriggered(struct perf_event *bp,
 				     struct perf_sample_data *data,
 				     struct pt_regs *regs)
 {
 	unsigned int address, destreg, data, type;
 	unsigned int res = 0;
 
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, regs >ARM_pc);
 
 	if (current >pid != previous_pid) {
 		pr_debug("\"%s\" (%ld) uses deprecated SWP{B} instruction\n",
 	fault = __do_page_fault(mm, addr, fsr, tsk);
 	up_read(&mm >mmap_sem);
 
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, addr);
 	if (fault & VM_FAULT_MAJOR)
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, regs, addr);
 	else if (fault & VM_FAULT_MINOR)
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, regs, addr);
 
 	/*
 	 * Handle the "normal" case first   VM_FAULT_MAJOR / VM_FAULT_MINOR
 	if (!mipspmu_event_set_period(event, hwc, idx))
 		return;
 
 	if (perf_event_overflow(event, data, regs))
 		mipspmu >disable_event(idx);
 }
 
 {
 	if ((opcode & OPCODE) == LL) {
 		perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
 				1, regs, 0);
 		return simulate_ll(regs, opcode);
 	}
 	if ((opcode & OPCODE) == SC) {
 		perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
 				1, regs, 0);
 		return simulate_sc(regs, opcode);
 	}
 
 		int rd = (opcode & RD) >> 11;
 		int rt = (opcode & RT) >> 16;
 		perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
 				1, regs, 0);
 		switch (rd) {
 		case 0:		/* CPU number */
 			regs >regs[rt] = smp_processor_id();
 {
 	if ((opcode & OPCODE) == SPEC0 && (opcode & FUNC) == SYNC) {
 		perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
 				1, regs, 0);
 		return 0;
 	}
 
 	unsigned long value;
 	unsigned int res;
 
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, 0);
 
 	/*
 	 * This load never faults.
 	mm_segment_t seg;
 
 	perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS,
 			1, regs, regs >cp0_badvaddr);
 	/*
 	 * Did we catch a fault trying to load an instruction?
 	 * Or are we running in MIPS16 mode?
 	}
 
       emul:
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, xcp, 0);
 	MIPS_FPU_EMU_INC_STATS(emulated);
 	switch (MIPSInst_OPCODE(ir)) {
 	case ldc1_op:{
 	 * the fault.
 	 */
 	fault = handle_mm_fault(mm, vma, address, write ? FAULT_FLAG_WRITE : 0);
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
 	if (unlikely(fault & VM_FAULT_ERROR)) {
 		if (fault & VM_FAULT_OOM)
 			goto out_of_memory;
 		BUG();
 	}
 	if (fault & VM_FAULT_MAJOR) {
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, regs, address);
 		tsk >maj_flt  ;
 	} else {
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, regs, address);
 		tsk >min_flt  ;
 	}
 
 #define PPC_WARN_EMULATED(type, regs)					\
 	do {								\
 		perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,		\
 			1, regs, 0);					\
 		__PPC_WARN_EMULATED(type);				\
 	} while (0)
 
 #define PPC_WARN_ALIGNMENT(type, regs)					\
 	do {								\
 		perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS,		\
 			1, regs, regs >dar);				\
 		__PPC_WARN_EMULATED(type);				\
 	} while (0)
 
  * here so there is no possibility of being interrupted.
  */
 static void record_and_restart(struct perf_event *event, unsigned long val,
 			       struct pt_regs *regs)
 {
 	u64 period = event >hw.sample_period;
 	s64 prev, delta, left;
 		if (event >attr.sample_type & PERF_SAMPLE_ADDR)
 			perf_get_data_addr(regs, &data.addr);
 
 		if (perf_event_overflow(event, &data, regs))
 			power_pmu_stop(event, 0);
 	}
 }
 		if ((int)val < 0) {
 			/* event has overflowed */
 			found = 1;
 			record_and_restart(event, val, regs);
 		}
 	}
 
  * here so there is no possibility of being interrupted.
  */
 static void record_and_restart(struct perf_event *event, unsigned long val,
 			       struct pt_regs *regs)
 {
 	u64 period = event >hw.sample_period;
 	s64 prev, delta, left;
 		perf_sample_data_init(&data, 0);
 		data.period = event >hw.last_period;
 
 		if (perf_event_overflow(event, &data, regs))
 			fsl_emb_pmu_stop(event, 0);
 	}
 }
 			if (event) {
 				/* event has overflowed */
 				found = 1;
 				record_and_restart(event, val, regs);
 			} else {
 				/*
 				 * Disabled counter is negative,
 }
 
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
 void ptrace_triggered(struct perf_event *bp,
 		      struct perf_sample_data *data, struct pt_regs *regs)
 {
 	struct perf_event_attr attr;
 		die("Weird page fault", regs, SIGSEGV);
 	}
 
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
 
 	/* When running in the kernel we expect faults to occur only to
 	 * addresses in user space.  All other faults represent errors in the
 	}
 	if (ret & VM_FAULT_MAJOR) {
 		current >maj_flt  ;
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1,
 				     regs, address);
 #ifdef CONFIG_PPC_SMLPAR
 		if (firmware_has_feature(FW_FEATURE_CMO)) {
 #endif
 	} else {
 		current >min_flt  ;
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1,
 				     regs, address);
 	}
 	up_read(&mm >mmap_sem);
 		goto out;
 
 	address = trans_exc_code & __FAIL_ADDR_MASK;
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
 	flags = FAULT_FLAG_ALLOW_RETRY;
 	if (access == VM_WRITE || (trans_exc_code & store_indication) == 0x400)
 		flags |= FAULT_FLAG_WRITE;
 	if (flags & FAULT_FLAG_ALLOW_RETRY) {
 		if (fault & VM_FAULT_MAJOR) {
 			tsk >maj_flt  ;
 			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1,
 				      regs, address);
 		} else {
 			tsk >min_flt  ;
 			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1,
 				      regs, address);
 		}
 		if (fault & VM_FAULT_RETRY) {
 	return 0;
 }
 
 void ptrace_triggered(struct perf_event *bp,
 		      struct perf_sample_data *data, struct pt_regs *regs)
 {
 	struct perf_event_attr attr;
 	 */
 	if (!expected) {
 		unaligned_fixups_notify(current, instruction, regs);
 		perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1,
 			      regs, address);
 	}
 
 		return error;
 	}
 
 	perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, regs, address);
 
 	destreg = (opcode >> 4) & 0x3f;
 	if (user_mode(regs)) {
 		return error;
 	}
 
 	perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, regs, address);
 
 	srcreg = (opcode >> 4) & 0x3f;
 	if (user_mode(regs)) {
 		return error;
 	}
 
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, address);
 
 	destreg = (opcode >> 4) & 0x3f;
 	if (user_mode(regs)) {
 		return error;
 	}
 
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, address);
 
 	srcreg = (opcode >> 4) & 0x3f;
 	if (user_mode(regs)) {
 	struct task_struct *tsk = current;
 	struct sh_fpu_soft_struct *fpu = &(tsk >thread.xstate >softfpu);
 
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, 0);
 
 	if (!(task_thread_info(tsk) >status & TS_USEDFPU)) {
 		/* initialize once. */
 	if ((regs >sr & SR_IMASK) != SR_IMASK)
 		local_irq_enable();
 
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
 
 	/*
 	 * If we're in an interrupt, have no user context or are running
 	}
 	if (fault & VM_FAULT_MAJOR) {
 		tsk >maj_flt  ;
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1,
 				     regs, address);
 	} else {
 		tsk >min_flt  ;
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1,
 				     regs, address);
 	}
 
 	/* Not an IO address, so reenable interrupts */
 	local_irq_enable();
 
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
 
 	/*
 	 * If we're in an interrupt or have no user
 
 	if (fault & VM_FAULT_MAJOR) {
 		tsk >maj_flt  ;
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1,
 				     regs, address);
 	} else {
 		tsk >min_flt  ;
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1,
 				     regs, address);
 	}
 
 		if (!sparc_perf_event_set_period(event, hwc, idx))
 			continue;
 
 		if (perf_event_overflow(event, &data, regs))
 			sparc_pmu_stop(event, 0);
 	}
 
 		unsigned long addr = compute_effective_address(regs, insn);
 		int err;
 
 		perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, regs, addr);
 		switch (dir) {
 		case load:
 			err = do_int_load(fetch_reg_addr(((insn>>25)&0x1f),
 		}
 
 		addr = compute_effective_address(regs, insn);
 		perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, regs, addr);
 		switch(dir) {
 		case load:
 			err = do_int_load(fetch_reg_addr(((insn>>25)&0x1f),
 
 		addr = compute_effective_address(regs, insn,
 						 ((insn >> 25) & 0x1f));
 		perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, regs, addr);
 		switch (asi) {
 		case ASI_NL:
 		case ASI_AIUPL:
 	int ret, i, rd = ((insn >> 25) & 0x1f);
 	int from_kernel = (regs >tstate & TSTATE_PRIV) != 0;
 
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, 0);
 	if (insn & 0x2000) {
 		maybe_flush_windows(0, 0, rd, from_kernel);
 		value = sign_extend_imm13(insn);
 	int asi = decode_asi(insn, regs);
 	int flag = (freg < 32) ? FPRS_DL : FPRS_DU;
 
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, 0);
 
 	save_and_clear_fpu();
 	current_thread_info() >xfsr[0] &= ~0x1c000;
 	int from_kernel = (regs >tstate & TSTATE_PRIV) != 0;
 	unsigned long *reg;
 
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, 0);
 
 	maybe_flush_windows(0, 0, rd, from_kernel);
 	reg = fetch_reg_addr(rd, regs);
 
 	if (tstate & TSTATE_PRIV)
 		die_if_kernel("lddfmna from kernel", regs);
 	perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, regs, sfar);
 	if (test_thread_flag(TIF_32BIT))
 		pc = (u32)pc;
 	if (get_user(insn, (u32 __user *) pc) !=  EFAULT) {
 
 	if (tstate & TSTATE_PRIV)
 		die_if_kernel("stdfmna from kernel", regs);
 	perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, regs, sfar);
 	if (test_thread_flag(TIF_32BIT))
 		pc = (u32)pc;
 	if (get_user(insn, (u32 __user *) pc) !=  EFAULT) {
 
 	BUG_ON(regs >tstate & TSTATE_PRIV);
 
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, 0);
 
 	if (test_thread_flag(TIF_32BIT))
 		pc = (u32)pc;
 	int retcode = 0;                               /* assume all succeed */
 	unsigned long insn;
 
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, 0);
 
 #ifdef DEBUG_MATHEMU
 	printk("In do_mathemu()... pc is %08lx\n", regs >pc);
 
 	if (tstate & TSTATE_PRIV)
 		die_if_kernel("unfinished/unimplemented FPop from kernel", regs);
 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, 0);
 	if (test_thread_flag(TIF_32BIT))
 		pc = (u32)pc;
 	if (get_user(insn, (u32 __user *) pc) !=  EFAULT) {
         if (in_atomic() || !mm)
                 goto no_context;
 
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
 
 	down_read(&mm >mmap_sem);
 
 	}
 	if (fault & VM_FAULT_MAJOR) {
 		current >maj_flt  ;
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, regs, address);
 	} else {
 		current >min_flt  ;
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, regs, address);
 	}
 	up_read(&mm >mmap_sem);
 	return;
 	if (in_atomic() || !mm)
 		goto intr_or_no_mm;
 
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
 
 	if (!down_read_trylock(&mm >mmap_sem)) {
 		if ((regs >tstate & TSTATE_PRIV) &&
 	}
 	if (fault & VM_FAULT_MAJOR) {
 		current >maj_flt  ;
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, regs, address);
 	} else {
 		current >min_flt  ;
 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, regs, address);
 	}
 	up_read(&mm >mmap_sem);
 
 		if (!x86_perf_event_set_period(event))
 			continue;
 
 		if (perf_event_overflow(event, &data, regs))
 			x86_pmu_stop(event, 0);
 	}
 
 
 		data.period = event >hw.last_period;
 
 		if (perf_event_overflow(event, &data, regs))
 			x86_pmu_stop(event, 0);
 	}
 
 	 */
 	perf_prepare_sample(&header, &data, event, &regs);
 
 	if (perf_output_begin(&handle, event, header.size * (top   at), 1))
 		return 1;
 
 	for (; at < top; at  ) {
 	else
 		regs.flags &= ~PERF_EFLAGS_EXACT;
 
 	if (perf_event_overflow(event, &data, &regs))
 		x86_pmu_stop(event, 0);
 }
 
 
 		if (!x86_perf_event_set_period(event))
 			continue;
 		if (perf_event_overflow(event, &data, regs))
 			x86_pmu_stop(event, 0);
 	}
 
 	return register_die_notifier(&kgdb_notifier);
 }
 
 static void kgdb_hw_overflow_handler(struct perf_event *event,
 		struct perf_sample_data *data, struct pt_regs *regs)
 {
 	struct task_struct *tsk = current;
 	return ret;
 }
 
 static void ptrace_triggered(struct perf_event *bp,
 			     struct perf_sample_data *data,
 			     struct pt_regs *regs)
 {
 	if (unlikely(error_code & PF_RSVD))
 		pgtable_bad(regs, error_code, address);
 
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
 
 	/*
 	 * If we're in an interrupt, have no user context or are running
 	if (flags & FAULT_FLAG_ALLOW_RETRY) {
 		if (fault & VM_FAULT_MAJOR) {
 			tsk >maj_flt  ;
 			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1,
 				      regs, address);
 		} else {
 			tsk >min_flt  ;
 			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1,
 				      regs, address);
 		}
 		if (fault & VM_FAULT_RETRY) {
 struct file;
 struct perf_sample_data;
 
 typedef void (*perf_overflow_handler_t)(struct perf_event *,
 					struct perf_sample_data *,
 					struct pt_regs *regs);
 
 	unsigned long			size;
 	void				*addr;
 	int				page;
 	int				sample;
 };
 
 				struct perf_event *event,
 				struct pt_regs *regs);
 
 extern int perf_event_overflow(struct perf_event *event,
 				 struct perf_sample_data *data,
 				 struct pt_regs *regs);
 
 
 extern struct jump_label_key perf_swevent_enabled[PERF_COUNT_SW_MAX];
 
 extern void __perf_sw_event(u32, u64, struct pt_regs *, u64);
 
 #ifndef perf_arch_fetch_caller_regs
 static inline void perf_arch_fetch_caller_regs(struct pt_regs *regs, unsigned long ip) { }
 }
 
 static __always_inline void
 perf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)
 {
 	struct pt_regs hot_regs;
 
 			perf_fetch_caller_regs(&hot_regs);
 			regs = &hot_regs;
 		}
 		__perf_sw_event(event_id, nr, regs, addr);
 	}
 }
 
 
 static inline void perf_event_task_sched_out(struct task_struct *task, struct task_struct *next)
 {
 	perf_sw_event(PERF_COUNT_SW_CONTEXT_SWITCHES, 1, NULL, 0);
 
 	__perf_event_task_sched_out(task, next);
 }
 
 extern int perf_output_begin(struct perf_output_handle *handle,
 			     struct perf_event *event, unsigned int size,
 			     int sample);
 extern void perf_output_end(struct perf_output_handle *handle);
 extern void perf_output_copy(struct perf_output_handle *handle,
 			     const void *buf, unsigned int len);
 static inline int perf_event_task_enable(void)				{ return  EINVAL; }
 
 static inline void
 perf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)	{ }
 static inline void
 perf_bp_event(struct perf_event *event, void *data)			{ }
 
 	}
 }
 
 static void perf_event_output(struct perf_event *event,
 				struct perf_sample_data *data,
 				struct pt_regs *regs)
 {
 
 	perf_prepare_sample(&header, data, event, regs);
 
 	if (perf_output_begin(&handle, event, header.size, 1))
 		goto exit;
 
 	perf_output_sample(&handle, &header, data, event);
 	int ret;
 
 	perf_event_header__init_id(&read_event.header, &sample, event);
 	ret = perf_output_begin(&handle, event, read_event.header.size, 0);
 	if (ret)
 		return;
 
 	perf_event_header__init_id(&task_event >event_id.header, &sample, event);
 
 	ret = perf_output_begin(&handle, event,
 				task_event >event_id.header.size, 0);
 	if (ret)
 		goto out;
 
 
 	perf_event_header__init_id(&comm_event >event_id.header, &sample, event);
 	ret = perf_output_begin(&handle, event,
 				comm_event >event_id.header.size, 0);
 
 	if (ret)
 		goto out;
 
 	perf_event_header__init_id(&mmap_event >event_id.header, &sample, event);
 	ret = perf_output_begin(&handle, event,
 				mmap_event >event_id.header.size, 0);
 	if (ret)
 		goto out;
 
 	perf_event_header__init_id(&throttle_event.header, &sample, event);
 
 	ret = perf_output_begin(&handle, event,
 				throttle_event.header.size, 0);
 	if (ret)
 		return;
 
  * Generic event overflow handling, sampling.
  */
 
 static int __perf_event_overflow(struct perf_event *event,
 				   int throttle, struct perf_sample_data *data,
 				   struct pt_regs *regs)
 {
 	if (events && atomic_dec_and_test(&event >event_limit)) {
 		ret = 1;
 		event >pending_kill = POLL_HUP;
 		event >pending_disable = 1;
 		irq_work_queue(&event >pending);
 	}
 
 	if (event >overflow_handler)
 		event >overflow_handler(event, data, regs);
 	else
 		perf_event_output(event, data, regs);
 
 	if (event >fasync && event >pending_kill) {
 		event >pending_wakeup = 1;
 		irq_work_queue(&event >pending);
 	}
 
 	return ret;
 }
 
 int perf_event_overflow(struct perf_event *event,
 			  struct perf_sample_data *data,
 			  struct pt_regs *regs)
 {
 	return __perf_event_overflow(event, 1, data, regs);
 }
 
 /*
 }
 
 static void perf_swevent_overflow(struct perf_event *event, u64 overflow,
 				    struct perf_sample_data *data,
 				    struct pt_regs *regs)
 {
 	struct hw_perf_event *hwc = &event >hw;
 		return;
 
 	for (; overflow; overflow  ) {
 		if (__perf_event_overflow(event, throttle,
 					    data, regs)) {
 			/*
 			 * We inhibit the overflow from happening when
 }
 
 static void perf_swevent_event(struct perf_event *event, u64 nr,
 			       struct perf_sample_data *data,
 			       struct pt_regs *regs)
 {
 	struct hw_perf_event *hwc = &event >hw;
 		return;
 
 	if (nr == 1 && hwc >sample_period == 1 && !event >attr.freq)
 		return perf_swevent_overflow(event, 1, data, regs);
 
 	if (local64_add_negative(nr, &hwc >period_left))
 		return;
 
 	perf_swevent_overflow(event, 0, data, regs);
 }
 
 static int perf_exclude_event(struct perf_event *event,
 }
 
 static void do_perf_sw_event(enum perf_type_id type, u32 event_id,
 				    u64 nr,
 				    struct perf_sample_data *data,
 				    struct pt_regs *regs)
 {
 
 	hlist_for_each_entry_rcu(event, node, head, hlist_entry) {
 		if (perf_swevent_match(event, type, event_id, data, regs))
 			perf_swevent_event(event, nr, data, regs);
 	}
 end:
 	rcu_read_unlock();
 	put_recursion_context(swhash >recursion, rctx);
 }
 
 void __perf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)
 {
 	struct perf_sample_data data;
 	int rctx;
 
 	perf_sample_data_init(&data, addr);
 
 	do_perf_sw_event(PERF_TYPE_SOFTWARE, event_id, nr, &data, regs);
 
 	perf_swevent_put_recursion_context(rctx);
 	preempt_enable_notrace();
 
 	hlist_for_each_entry_rcu(event, node, head, hlist_entry) {
 		if (perf_tp_event_match(event, &data, regs))
 			perf_swevent_event(event, count, &data, regs);
 	}
 
 	perf_swevent_put_recursion_context(rctx);
 	perf_sample_data_init(&sample, bp >attr.bp_addr);
 
 	if (!bp >hw.state && !perf_exclude_event(bp, regs))
 		perf_swevent_event(bp, 1, &sample, regs);
 }
 #endif
 
 
 	if (regs && !perf_exclude_event(event, regs)) {
 		if (!(event >attr.exclude_idle && current >pid == 0))
 			if (perf_event_overflow(event, &data, regs))
 				ret = HRTIMER_NORESTART;
 	}
 
 	void				*data_pages[0];
 };
 
 extern void rb_free(struct ring_buffer *rb);
 extern struct ring_buffer *
 rb_alloc(int nr_pages, long watermark, int cpu, int flags);
 {
 	atomic_set(&handle >rb >poll, POLL_IN);
 
 	handle >event >pending_wakeup = 1;
 	irq_work_queue(&handle >event >pending);
 }
 
 /*
 
 int perf_output_begin(struct perf_output_handle *handle,
 		      struct perf_event *event, unsigned int size,
 		      int sample)
 {
 	struct ring_buffer *rb;
 	unsigned long tail, offset, head;
 
 	handle >rb	= rb;
 	handle >event	= event;
 	handle >sample	= sample;
 
 	if (!rb >nr_pages)
 
 	if (task_cpu(p) != new_cpu) {
 		p >se.nr_migrations  ;
 		perf_sw_event(PERF_COUNT_SW_CPU_MIGRATIONS, 1, NULL, 0);
 	}
 
 	__set_task_cpu(p, new_cpu);
 };
 
 /* Callback function for perf event subsystem */
 static void watchdog_overflow_callback(struct perf_event *event,
 		 struct perf_sample_data *data,
 		 struct pt_regs *regs)
 {
 MODULE_PARM_DESC(ksym, "Kernel symbol to monitor; this module will report any"
 			" write operations on the kernel symbol");
 
 static void sample_hbp_handler(struct perf_event *bp,
 			       struct perf_sample_data *data,
 			       struct pt_regs *regs)
 {
