CVE Number : CVE-2021-45469
Commit Message : 
f2fs: fix to keep isolation of atomic writedev
Commit Details : 
As Yi Chen reported, there is a potential race case described as below:

Thread A			Thread B
- f2fs_ioc_start_atomic_write
				- mkwrite
				 - set_page_dirty
				  - f2fs_set_page_private(page, 0)
 - set_inode_flag(FI_ATOMIC_FILE)
				- mkwrite same page
				 - set_page_dirty
				  - f2fs_register_inmem_page
				   - f2fs_set_page_private(ATOMIC_WRITTEN_PAGE)
				     failed due to PagePrivate flag has been set
				   - list_add_tail
				- truncate_inode_pages
				 - f2fs_invalidate_page
				  - clear page private but w/o remove it from
				    inmem_list
				 - set page->mapping to NULL
- f2fs_ioc_commit_atomic_write
 - __f2fs_commit_inmem_pages
   - __revoke_inmem_pages
    - f2fs_put_page panic as page->mapping is NULL

The root cause is we missed to keep isolation of atomic write in the case
of start_atomic_write vs mkwrite, let start_atomic_write helds i_mmap_sem
lock to avoid this issue.

Reported-by: Yi Chen <chenyi77@huawei.com>
Signed-off-by: Chao Yu <chao@kernel.org>

Before patch : 
 		goto out;
 
 	down_write(&F2FS_I(inode) >i_gc_rwsem[WRITE]);
 
 	/*
 	 * Should wait end_io to count F2FS_WB_CP_DATA correctly by
 			  inode >i_ino, get_dirty_pages(inode));
 	ret = filemap_write_and_wait_range(inode >i_mapping, 0, LLONG_MAX);
 	if (ret) {
 		up_write(&F2FS_I(inode) >i_gc_rwsem[WRITE]);
 		goto out;
 	}
 	/* add inode in inmem_list first and set atomic_file */
 	set_inode_flag(inode, FI_ATOMIC_FILE);
 	clear_inode_flag(inode, FI_ATOMIC_REVOKE_REQUEST);
 	up_write(&F2FS_I(inode) >i_gc_rwsem[WRITE]);
 
 	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
 	struct f2fs_inode_info *fi = F2FS_I(inode);
 
 	do {
 		mutex_lock(&fi >inmem_lock);
 		if (list_empty(&fi >inmem_pages)) {
 			fi >i_gc_failures[GC_FAILURE_ATOMIC] = 0;
 			spin_unlock(&sbi >inode_lock[ATOMIC_FILE]);
 
 			mutex_unlock(&fi >inmem_lock);
 			break;
 		}
 		__revoke_inmem_pages(inode, &fi >inmem_pages,
 						true, false, true);
 		mutex_unlock(&fi >inmem_lock);
 	} while (1);
 }
 
 	f2fs_balance_fs(sbi, true);
 
 	down_write(&fi >i_gc_rwsem[WRITE]);
 
 	f2fs_lock_op(sbi);
 	set_inode_flag(inode, FI_ATOMIC_COMMIT);
 	clear_inode_flag(inode, FI_ATOMIC_COMMIT);
 
 	f2fs_unlock_op(sbi);
 	up_write(&fi >i_gc_rwsem[WRITE]);
 
 	return err;
After patch : 
 		goto out;
 
 	down_write(&F2FS_I(inode) >i_gc_rwsem[WRITE]);
 	filemap_invalidate_lock(inode >i_mapping);
 
 	/*
 	 * Should wait end_io to count F2FS_WB_CP_DATA correctly by
 			  inode >i_ino, get_dirty_pages(inode));
 	ret = filemap_write_and_wait_range(inode >i_mapping, 0, LLONG_MAX);
 	if (ret) {
 		filemap_invalidate_unlock(inode >i_mapping);
 		up_write(&F2FS_I(inode) >i_gc_rwsem[WRITE]);
 		goto out;
 	}
 	/* add inode in inmem_list first and set atomic_file */
 	set_inode_flag(inode, FI_ATOMIC_FILE);
 	clear_inode_flag(inode, FI_ATOMIC_REVOKE_REQUEST);
 	filemap_invalidate_unlock(inode >i_mapping);
 	up_write(&F2FS_I(inode) >i_gc_rwsem[WRITE]);
 
 	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
 	struct f2fs_inode_info *fi = F2FS_I(inode);
 
 	do {
 		filemap_invalidate_lock(inode >i_mapping);
 		mutex_lock(&fi >inmem_lock);
 		if (list_empty(&fi >inmem_pages)) {
 			fi >i_gc_failures[GC_FAILURE_ATOMIC] = 0;
 			spin_unlock(&sbi >inode_lock[ATOMIC_FILE]);
 
 			mutex_unlock(&fi >inmem_lock);
 			filemap_invalidate_unlock(inode >i_mapping);
 			break;
 		}
 		__revoke_inmem_pages(inode, &fi >inmem_pages,
 						true, false, true);
 		mutex_unlock(&fi >inmem_lock);
 		filemap_invalidate_unlock(inode >i_mapping);
 	} while (1);
 }
 
 	f2fs_balance_fs(sbi, true);
 
 	down_write(&fi >i_gc_rwsem[WRITE]);
 	filemap_invalidate_lock(inode >i_mapping);
 
 	f2fs_lock_op(sbi);
 	set_inode_flag(inode, FI_ATOMIC_COMMIT);
 	clear_inode_flag(inode, FI_ATOMIC_COMMIT);
 
 	f2fs_unlock_op(sbi);
 
 	filemap_invalidate_unlock(inode >i_mapping);
 	up_write(&fi >i_gc_rwsem[WRITE]);
 
 	return err;
